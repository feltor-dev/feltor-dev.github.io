<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<title>Discontinuous Galerkin Library: Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?.../MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="menubar.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
    <!--My own title area-->
  <ul class="menubar">
      <!-- <li><a href="../../../index.html">/</a><li>-->
    <li><a href="../../dg/html/modules.html">dg</a></li>
    <li><a href="../../geometries/html/modules.html">dg::geo</a></li>
    <li><a href="../../file/html/modules.html">dg::file</a></li>
    <li><a href="../../exblas/html/namespacedg_1_1exblas.html">dg::exblas</a></li>
  </ul>
  <!--End My own title area-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Discontinuous Galerkin Library
   </div>
   <div id="projectbrief">Container free algorithms and discontinuous Galerkin numerical methods in &quot;dg/algorithm.h&quot;</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('index.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Introduction </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="pdf"></a>
Introduction to discontinuous Galerkin methods</h1>
<p>Here is a pdf document explaining the fundamentals of discontinuous Galerkin methods</p><ul>
<li><a href="./dg_introduction.pdf" target="_blank">Introduction to dg methods</a></li>
</ul>
<h1><a class="anchor" id="dispatch"></a>
The Level 1 dispatch system</h1>
<p>Let us first define some nomenclature to ease the following discussion</p><ul>
<li><em>Scalar:</em> A template parameter T is a Scalar if <code> typename dg::TensorTraits&lt;T&gt;::tensor_category </code> exists and derives from <code><a class="el" href="structdg_1_1_any_scalar_tag.html" title="Scalar Tag base class, indicates the basic Scalar Tensor concept.">dg::AnyScalarTag</a></code> </li>
<li><em>Vector:</em> A template parameter T is a Vector if it is not a Scalar and if <code> typename dg::TensorTraits&lt;T&gt;::tensor_category </code> exists and derives from <code><a class="el" href="structdg_1_1_any_vector_tag.html" title="Vector Tag base class, indicates the basic Vector/container concept.">dg::AnyVectorTag</a></code> </li>
<li><em>Matrix:</em> A template parameter T is a Matrix if it is not a Scalar or Vector and if <code> typename dg::TensorTraits&lt;T&gt;::tensor_category </code> exists and derives from <code><a class="el" href="structdg_1_1_any_matrix_tag.html">dg::AnyMatrixTag</a></code> </li>
<li><em>execution</em> <em>policy:</em> A template parameter T has an execution policy if <code> typename dg::TensorTraits&lt;T&gt;::execution_policy </code> exists and derives from <code><a class="el" href="structdg_1_1_any_policy_tag.html" title="Execution Policy base class.">dg::AnyPolicyTag</a></code>, The execution policy is <em>trivial</em> if it is <code><a class="el" href="structdg_1_1_any_policy_tag.html" title="Execution Policy base class.">dg::AnyPolicyTag</a></code> </li>
<li><em>value</em> <em>type</em> : A template parameter T has a value type if <code> typename dg::TensorTraits&lt;T&gt;::value_type </code> exists</li>
<li><em>compatible:</em> Two vectors are compatible if their tensor_categories both derive from the same base class that itself derives from but is not equal to <code><a class="el" href="structdg_1_1_any_vector_tag.html" title="Vector Tag base class, indicates the basic Vector/container concept.">dg::AnyVectorTag</a></code>, Two execution policies are compatible if they are equal or if at least one of them is trivial.</li>
<li><em>promote:</em> A Scalar can be promoted to a Vector with all elements equal to the value of the Scalar. A Vector can be promoted to a Matrix with the Vector being the diagonal and all other elements zero.</li>
</ul>
<p>When dispatching level 1 functions we distinguish between three classes of functions: trivially parallel (<code><a class="el" href="group__blas1.html#ga7386b5cb0144d5364b8ea8c8ce1482a5" title="; Customizable and generic blas1 function">dg::blas1::subroutine</a></code> and related <code><a class="el" href="namespacedg_1_1blas1.html" title="BLAS Level 1 routines.">dg::blas1</a></code> functions), global communication (<code><a class="el" href="group__blas1.html#ga7b2aa636a0fe0c7d3298b9f869658919" title="Binary reproducible Euclidean dot product between two vectors">dg::blas1::dot</a></code> and <code><a class="el" href="group__blas2.html#gaaafba91956e948b0ea53f30889a3c20d" title="; Binary reproducible general dot product">dg::blas2::dot</a></code>) and local communication (<code><a class="el" href="group__blas2.html#gac3840941bd2bb1d64c6ca3fef1f1b960">dg::blas2::symv</a></code>).</p>
<h2><a class="anchor" id="dispatch_evaluate"></a>
The evaluate function</h2>
<p>The execution of <code><a class="el" href="group__blas1.html#ga7386b5cb0144d5364b8ea8c8ce1482a5" title="; Customizable and generic blas1 function">dg::blas1::subroutine</a></code> with a Functor called <code>routine</code> (and all related <code><a class="el" href="namespacedg_1_1blas1.html" title="BLAS Level 1 routines.">dg::blas1</a></code> functions) is equivalent to the following:</p><ol type="1">
<li>Assert the following prerequisites:<ol type="a">
<li>All template parameter types must be either Scalars or Vectors and have an execution policy and a value type</li>
<li>All Vectors and all execution policies must be mutually compatible</li>
<li>All Vectors must contain the same number of elements (equal sizes)</li>
<li>The number of template parameters must be equal to the number of parameters in the <code>routine</code> </li>
<li>The value type of every template parameter must be convertible to the respective parameter type in the <code>routine</code> </li>
</ol>
</li>
<li>If all types are Scalars, apply the <code>routine</code> and return</li>
<li>If at least one type is a Vector, then all Scalars are promoted to this type with the same size, communicator and execution policy</li>
<li>Check the base class of the tensor_category:<ol type="a">
<li>If <code><a class="el" href="structdg_1_1_shared_vector_tag.html" title="Indicate a contiguous chunk of shared memory.">dg::SharedVectorTag</a></code>, check execution policy and dispatch to respective implementation (The implementation just loops over all elements in the vectors and applies the <code>routine</code>)</li>
<li>If <code><a class="el" href="structdg_1_1_m_p_i_vector_tag.html" title="A distributed vector contains a data container and a MPI communicator.">dg::MPIVectorTag</a></code>, access the underlying data and recursively call <code><a class="el" href="group__blas1.html#ga7386b5cb0144d5364b8ea8c8ce1482a5" title="; Customizable and generic blas1 function">dg::blas1::subroutine</a></code> (and start again at 1)</li>
<li>If <code><a class="el" href="structdg_1_1_recursive_vector_tag.html" title="This tag indicates composition/recursion.">dg::RecursiveVectorTag</a></code>, loop over all elements and recursively call <code><a class="el" href="group__blas1.html#ga7386b5cb0144d5364b8ea8c8ce1482a5" title="; Customizable and generic blas1 function">dg::blas1::subroutine</a></code> for all elements (and start again at 1)</li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="dispatch_dot"></a>
The dot function</h2>
<p>The execution of <code><a class="el" href="group__blas1.html#ga7b2aa636a0fe0c7d3298b9f869658919" title="Binary reproducible Euclidean dot product between two vectors">dg::blas1::dot</a></code> and <code><a class="el" href="group__blas2.html#gaaafba91956e948b0ea53f30889a3c20d" title="; Binary reproducible general dot product">dg::blas2::dot</a></code> is equivalent to the following:</p><ol type="1">
<li>Assert the following prerequisites:<ol type="a">
<li>All template parameter types must be either Scalars or Vectors and have an execution policy and a value type</li>
<li>All Vectors and all execution policies must be mutually compatible</li>
<li>All Vectors must contain the same number of elements (equal sizes)</li>
</ol>
</li>
<li>If all types are Scalars, multiply and return</li>
<li>If at least one type is a Vector, then all Scalars are promoted to this type with the same size, communicator and execution policy</li>
<li>Check the base class of the tensor_category:<ol type="a">
<li>If <code><a class="el" href="structdg_1_1_shared_vector_tag.html" title="Indicate a contiguous chunk of shared memory.">dg::SharedVectorTag</a></code>, check execution policy and dispatch to respective implementation (The implementation multiplies and accumulates all elements in the vectors). Return the result.</li>
<li>If <code><a class="el" href="structdg_1_1_m_p_i_vector_tag.html" title="A distributed vector contains a data container and a MPI communicator.">dg::MPIVectorTag</a></code>, assert that the vector MPI-communicators are <code>congruent</code> (same process group, different context) or <code>ident</code> (same process group, same context). Then, access the underlying data and recursively call <code>dot</code> (and start again at 1). Accumulate the result among participating processes and return.</li>
<li>If <code><a class="el" href="structdg_1_1_recursive_vector_tag.html" title="This tag indicates composition/recursion.">dg::RecursiveVectorTag</a></code>, loop over all elements and recursively call <code>dot</code> for all elements (and start again at 1). Accumulate the results and return.</li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="dispatch_symv"></a>
The symv function</h2>
<p>The execution of the <code><a class="el" href="group__blas2.html#gac3840941bd2bb1d64c6ca3fef1f1b960">dg::blas2::symv</a></code> (and <code><a class="el" href="group__blas2.html#ga1b7e3b58697b6e93169eebbda63f3ed3" title="; (alias for symv)">dg::blas2::gemv</a></code>) functions is hard to discribe in general since each matrix class has individual prerequisites and execution paths. Still, we can identify some general rules:</p><ol type="1">
<li>The Matrix type can be either a Scalar (promotes to Scalar times the Unit Matrix), a Vector (promotes to a diagonal Matrix) or a Matrix</li>
<li>If the Matrix is either a Scalar or a Vector and the remaining types do not have the <code><a class="el" href="structdg_1_1_recursive_vector_tag.html" title="This tag indicates composition/recursion.">dg::RecursiveVectorTag</a></code> tensor category, then <code><a class="el" href="group__blas2.html#gac3840941bd2bb1d64c6ca3fef1f1b960">dg::blas2::symv</a></code> is equivalent to <code><a class="el" href="group__blas1.html#gad2dcd7cffe760b7bacfdd7647d86e602">dg::blas1::pointwiseDot</a></code> </li>
<li>If the Matrix has the <code><a class="el" href="structdg_1_1_self_made_matrix_tag.html" title="Indicates that the type has a member function with the same name and interface (up to the matrix itse...">dg::SelfMadeMatrixTag</a></code> tensor category, then all parameters are immediately forwarded to the <code>symv</code> member function. No asserts are performed and none of the following applies.</li>
<li>The container template parameters must be Vectors or Scalars and must have compatible execution policies. Vectors must be compatible.</li>
<li>If the tensor category of the Vectors is <code><a class="el" href="structdg_1_1_recursive_vector_tag.html" title="This tag indicates composition/recursion.">dg::RecursiveVectorTag</a></code> and the tensor category of the Matrix is not, then the <code><a class="el" href="group__blas2.html#gac3840941bd2bb1d64c6ca3fef1f1b960">dg::blas2::symv</a></code> is recursively called with the Matrix on all elements of the Vectors.</li>
</ol>
<h1><a class="anchor" id="mpi_backend"></a>
The MPI interface</h1>
<dl class="section note"><dt>Note</dt><dd>The mpi backend is activated by including <code>mpi.h</code> before any other feltor header file </dd></dl>
<h2><a class="anchor" id="mpi_vector"></a>
MPI Vectors and the blas functions</h2>
<p>In Feltor each mpi process gets an equally sized chunk of a vector. The corresponding structure in FELTOR is the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="mpi Vector class">dg::MPI_Vector</a></code>, which is nothing but a wrapper around a container type object and a <code>MPI_Comm</code>. With this the <code><a class="el" href="namespacedg_1_1blas1.html" title="BLAS Level 1 routines.">dg::blas1</a></code> functions can readily be implemented by just redirecting to the implementation for the container type. The only functions that need additional communication are the <code><a class="el" href="group__blas1.html#ga7b2aa636a0fe0c7d3298b9f869658919" title="Binary reproducible Euclidean dot product between two vectors">dg::blas1::dot</a></code> and <code><a class="el" href="group__blas2.html#gaaafba91956e948b0ea53f30889a3c20d" title="; Binary reproducible general dot product">dg::blas2::dot</a></code> functions (<code>MPI_Allreduce</code>).</p>
<h2><a class="anchor" id="mpi_matrix"></a>
MPI Matrices and the symv function</h2>
<p>Contrary to a vector a matrix can be distributed among processes in two ways: <em>row-wise</em> and <em>column-wise</em>. When we implement a matrix-vector multiplication the order of communication and computation depends on the distribution of the matrix.</p>
<h3><a class="anchor" id="mpi_row"></a>
Row distributed matrices</h3>
<p>In a row-distributed matrix each process holds the rows of the matrix that correspond to the portion of the <code>MPI_Vector</code> it holds. Every process thus holds the same amount of rows. When we implement a matrix-vector multiplication each process first has to gather all the elements of the input vector it needs to be able to compute the elements of its output. In general this requires MPI communication. (read the documentation of <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code> for more info of how global scatter/gather operations work). After the elements have been gathered into a buffer the local matrix-vector multiplications can be executed. Formally, the gather operation can be written as a matrix \(G\) of \(1&#39;\)s and \(0&#39;\)s and we write. </p><p class="formulaDsp">
\[ M v = R\cdot G v \]
</p>
<p> where \(R\) is the row-distributed matrix with modified indices into a buffer vector and \(G\) is the gather matrix, in which the MPI-communication takes place. In this way we achieve a simple split between communication \( w=Gv\) and computation \( Rw\). Since the computation of \( R w\) is entirely local we can reuse the existing implementation for shared memory systems. Correspondingly, we define the structure <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class.">dg::MPIDistMat</a></code> as a simple a wrapper around a <code>LocalMatrix</code> type object and an instance of a <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>.</p>
<h3><a class="anchor" id="mpi_column"></a>
Column distributed matrices</h3>
<p>In a column-distributed matrix each process holds the columns of the matrix that correspond to the portion of the <code>MPI_Vector</code> it holds. Each process thus holds the same amount of columns. In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already has all vector elements it needs. However the resulting elements have to be communicated back to the process they belong to. Furthermore, a process has to sum all elements it receives from other processes on the same index. This is a scatter and reduce operation and it can be written as a scatter matrix \(S\) (s.a. <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>). </p><p class="formulaDsp">
\[ M v= S\cdot C v \]
</p>
<p> where \(S\) is the scatter matrix and \(C\) is the column distributed matrix with modified indices. Again, we can reuse our shared memory algorithms to implement the local matrix-vector operation \( w=Cv\) before the communication step \( S w\). This is also implemented in <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class.">dg::MPIDistMat</a></code>.</p>
<h3><a class="anchor" id="mpi_row_col"></a>
Row and Column distributed</h3>
<p>The <code><a class="el" href="structdg_1_1_row_col_dist_mat.html" title="Distributed memory matrix class, asynchronous communication.">dg::RowColDistMat</a></code> goes one step further on a row distributed matrix and separates the matrix \( R = R_{inner} + R_{outer}\) into a part that can be computed entirely on the local process and a part that needs communication. This enables the implementation of overlapping communication and computation. (s.a. <code><a class="el" href="structdg_1_1_nearest_neighbor_comm.html" title="Communicator for asynchronous nearest neighbor communication.">dg::NearestNeighborComm</a></code>) </p>
<h3><a class="anchor" id="mpi_transpose"></a>
Transposition</h3>
<p>It turns out that a row-distributed matrix can be transposed by transposition of both the local matrix and the gather matrix (s.a. <code><a class="el" href="group__misc.html#gaf9b1ec92041d094f14481ad8169d12c4" title="Transpose vector.">dg::transpose</a></code>): </p><p class="formulaDsp">
\[ M^\mathrm{T} = G^\mathrm{T} R^\mathrm{T}\]
</p>
<p> The result is then a column distributed matrix. Analogously, the transpose of a column distributed matrix is a row-distributed matrix. </p>
<h3><a class="anchor" id="mpi_create"></a>
Creation</h3>
<p>You can create an MPI row-distributed matrix if you know the global column indices by our <code><a class="el" href="group__mpi__structures.html#gaf94a6a2fb51d6f795e0e9c81ae03fcdb" title="Convert a matrix with local row and global column indices to a row distributed MPI matrix.">dg::convert</a></code> function. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Tue Feb 16 2021 01:15:31 for Discontinuous Galerkin Library by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
