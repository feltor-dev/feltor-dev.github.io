<!-- HTML header for doxygen 1.9.3-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.3"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Discontinuous Galerkin Library: MPI backend</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?.../MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="menubar.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
    <!--My own title area-->
  <ul class="menubar">
      <!-- <li><a href="../../../index.html">/</a><li>-->
    <li><a href="../../dg/html/modules.html">dg</a></li>
    <li><a href="../../geometries/html/modules.html">dg::geo</a></li>
    <li><a href="../../file/html/modules.html">dg::file</a></li>
    <li><a href="../../exblas/html/namespacedg_1_1exblas.html">dg::exblas</a></li>
  </ul>
  <!--End My own title area-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Discontinuous Galerkin Library
   </div>
   <div id="projectbrief">#include &quot;dg/algorithm.h&quot;</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.3 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('group__mpi__structures.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">MPI backend<div class="ingroups"><a class="el" href="group__backend.html">Level 1: Vectors, Matrices and basic operations</a></div></div></div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Collaboration diagram for MPI backend:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="group__mpi__structures.svg" width="332" height="51"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_bijective_comm.html">dg::BijectiveComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform bijective gather and its transpose (scatter) operation across processes on distributed vectors using mpi.  <a href="structdg_1_1_bijective_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_surjective_comm.html">dg::SurjectiveComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform surjective gather and its transpose (scatter) operation across processes on distributed vectors using mpi.  <a href="structdg_1_1_surjective_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_general_comm.html">dg::GeneralComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform general gather and its transpose (scatter) operation across processes on distributed vectors using mpi.  <a href="structdg_1_1_general_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1a_communicator.html">dg::aCommunicator&lt; LocalContainer &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Struct that performs collective scatter and gather operations across processes on distributed vectors using MPI.  <a href="structdg_1_1a_communicator.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_row_col_dist_mat.html">dg::RowColDistMat&lt; LocalMatrixInner, LocalMatrixOuter, Collective &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory matrix class, asynchronous communication.  <a href="structdg_1_1_row_col_dist_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i_dist_mat.html">dg::MPIDistMat&lt; LocalMatrix, Collective &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory matrix class.  <a href="structdg_1_1_m_p_i_dist_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i___vector.html">dg::MPI_Vector&lt; container &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">mpi Vector class  <a href="structdg_1_1_m_p_i___vector.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_nearest_neighbor_comm.html">dg::NearestNeighborComm&lt; Index, Buffer, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Communicator for asynchronous nearest neighbor communication.  <a href="structdg_1_1_nearest_neighbor_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="enum-members" name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">dg::dist_type</a> { <a class="el" href="group__mpi__structures.html#gga521b6d6524a6d391c746ddc9a1ad5571a99ba8c51e9d19b4535a343c4b471a562">dg::row_dist</a> =0
, <a class="el" href="group__mpi__structures.html#gga521b6d6524a6d391c746ddc9a1ad5571a4bdc977f2b0fcd3ad828d30c7fd3fb90">dg::col_dist</a> =1
 }</td></tr>
<tr class="memdesc:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of distribution of MPI distributed matrices.  <a href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">More...</a><br /></td></tr>
<tr class="separator:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:gab037631f28f2fa5d19fd58893c89e986"><td class="memTemplParams" colspan="2">template&lt;class ConversionPolicy , class real_type &gt; </td></tr>
<tr class="memitem:gab037631f28f2fa5d19fd58893c89e986"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="group__typedefs.html#ga383790a28074c1998e9abef496429b99">dg::MIHMatrix_t</a>&lt; real_type &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="group__mpi__structures.html#gab037631f28f2fa5d19fd58893c89e986">dg::convert</a> (const <a class="el" href="group__typedefs.html#ga405d290c77c92765dcddb324522d8409">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;global, const ConversionPolicy &amp;policy)</td></tr>
<tr class="memdesc:gab037631f28f2fa5d19fd58893c89e986"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convert a matrix with local row and global column indices to a row distributed MPI matrix.  <a href="group__mpi__structures.html#gab037631f28f2fa5d19fd58893c89e986">More...</a><br /></td></tr>
<tr class="separator:gab037631f28f2fa5d19fd58893c89e986"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p >In this section the blas functions are implemented for the MPI+X hardware architectures, where X is e.g. CPU, GPU, accelerator cards... The general idea to achieve this is to separate global communication from local computations and thus readily reuse the existing, optimized library for the local part.</p>
<h1><a class="anchor" id="mpi_backend"></a>
The MPI interface</h1>
<dl class="section note"><dt>Note</dt><dd>The mpi backend is activated by including <code>mpi.h</code> before any other feltor header file </dd></dl>
<h2><a class="anchor" id="mpi_vector"></a>
MPI Vectors and the blas functions</h2>
<p >In Feltor each mpi process usualy gets an equally sized chunk of a vector. In particular the <code><a class="el" href="structdg_1_1a_real_m_p_i_topology2d.html" title="2D MPI abstract grid class">dg::aRealMPITopology2d</a></code> and <code><a class="el" href="structdg_1_1a_real_m_p_i_topology3d.html" title="3D MPI Grid class">dg::aRealMPITopology3d</a></code> classes represent the standard Cartesian process and point distribution (meaning every process gets an equally sized 2d / 3d box out of the global domain ). The corresponding mpi vector structure in FELTOR is the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="mpi Vector class">dg::MPI_Vector</a></code>, which is nothing but a wrapper around a container type object and a <code>MPI_Comm</code>. With this the <code><a class="el" href="namespacedg_1_1blas1.html" title="BLAS Level 1 routines.">dg::blas1</a></code> functions can readily be implemented by just redirecting to the implementation for the container type. The only functions that need additional communication are the <code><a class="el" href="group__blas1.html#gaf00778ded011a9f6e8b885924d9306ee" title="Binary reproducible Euclidean dot product between two vectors">dg::blas1::dot</a></code> and <code><a class="el" href="group__blas2.html#gaee53d6bf6aa0ca3059a49ea696d4d158" title="; Binary reproducible general dot product">dg::blas2::dot</a></code> functions (<code>MPI_Allreduce</code>). </p>
<h2><a class="anchor" id="mpi_matrix"></a>
MPI Matrices and the symv function</h2>
<p >Contrary to a vector a matrix can be distributed among processes in two ways: <em>row-wise</em> and <em>column-wise</em>. When we implement a matrix-vector multiplication the order of communication and computation depends on the distribution of the matrix.</p>
<h3><a class="anchor" id="mpi_row"></a>
Row distributed matrices</h3>
<p >In a row-distributed matrix each process holds the rows of the matrix that correspond to the portion of the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="mpi Vector class">MPI_Vector</a></code> it holds. Every process thus holds the same amount of rows. When we implement a matrix-vector multiplication each process first has to gather all the elements of the input vector it needs to be able to compute the elements of its output. In general this requires MPI communication. (read the documentation of <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code> for more info of how global scatter/gather operations work). After the elements have been gathered into a buffer the local matrix-vector multiplications can be executed. Formally, the gather operation can be written as a matrix \(G\) of \(1&#39;\)s and \(0&#39;\)s and we write. </p><p class="formulaDsp">
\[ M v = R\cdot G v \]
</p>
<p> where \(R\) is the row-distributed matrix with modified indices into a buffer vector and \(G\) is the gather matrix, in which the MPI-communication takes place. In this way we achieve a simple split between communication \( w=Gv\) and computation \( Rw\). Since the computation of \( R w\) is entirely local we can reuse the existing implementation for shared memory systems. Correspondingly, we define the structure <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class.">dg::MPIDistMat</a></code> as a simple a wrapper around a <code>LocalMatrix</code> type object and an instance of a <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>.</p>
<h3><a class="anchor" id="mpi_column"></a>
Column distributed matrices</h3>
<p >In a column-distributed matrix each process holds the columns of the matrix that correspond to the portion of the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="mpi Vector class">MPI_Vector</a></code> it holds. Each process thus holds the same amount of columns. In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already has all vector elements it needs. However the resulting elements have to be communicated back to the process they belong to. Furthermore, a process has to sum all elements it receives from other processes on the same index. This is a scatter and reduce operation and it can be written as a scatter matrix \(S\) (s.a. <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>). </p><p class="formulaDsp">
\[ M v= S\cdot C v \]
</p>
<p> where \(S\) is the scatter matrix and \(C\) is the column distributed matrix with modified indices. Again, we can reuse our shared memory algorithms to implement the local matrix-vector operation \( w=Cv\) before the communication step \( S w\). This is also implemented in <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class.">dg::MPIDistMat</a></code>.</p>
<h3><a class="anchor" id="mpi_row_col"></a>
Row and Column distributed</h3>
<p >The <code><a class="el" href="structdg_1_1_row_col_dist_mat.html" title="Distributed memory matrix class, asynchronous communication.">dg::RowColDistMat</a></code> goes one step further on a row distributed matrix and separates the matrix \( R = R_{inner} + R_{outer}\) into a part that can be computed entirely on the local process and a part that needs communication. This enables the implementation of overlapping communication and computation. (s.a. <code><a class="el" href="structdg_1_1_nearest_neighbor_comm.html" title="Communicator for asynchronous nearest neighbor communication.">dg::NearestNeighborComm</a></code>) </p>
<h3><a class="anchor" id="mpi_transpose"></a>
Transposition</h3>
<p >It turns out that a row-distributed matrix can be transposed by transposition of both the local matrix and the gather matrix (s.a. <code><a class="el" href="group__misc.html#gaf9b1ec92041d094f14481ad8169d12c4" title="Transpose vector.">dg::transpose</a></code>): </p><p class="formulaDsp">
\[ M^\mathrm{T} = G^\mathrm{T} R^\mathrm{T}\]
</p>
<p> The result is then a column distributed matrix. Analogously, the transpose of a column distributed matrix is a row-distributed matrix. </p>
<h3><a class="anchor" id="mpi_create"></a>
Creation</h3>
<p >You can create an MPI row-distributed matrix if you know the global column indices by our <code><a class="el" href="group__mpi__structures.html#gab037631f28f2fa5d19fd58893c89e986" title="Convert a matrix with local row and global column indices to a row distributed MPI matrix.">dg::convert</a></code> function. </p>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a id="ga521b6d6524a6d391c746ddc9a1ad5571" name="ga521b6d6524a6d391c746ddc9a1ad5571"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga521b6d6524a6d391c746ddc9a1ad5571">&#9670;&nbsp;</a></span>dist_type</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">dg::dist_type</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Type of distribution of MPI distributed matrices. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a id="gga521b6d6524a6d391c746ddc9a1ad5571a99ba8c51e9d19b4535a343c4b471a562" name="gga521b6d6524a6d391c746ddc9a1ad5571a99ba8c51e9d19b4535a343c4b471a562"></a>row_dist&#160;</td><td class="fielddoc"><p >Row distributed. </p>
</td></tr>
<tr><td class="fieldname"><a id="gga521b6d6524a6d391c746ddc9a1ad5571a4bdc977f2b0fcd3ad828d30c7fd3fb90" name="gga521b6d6524a6d391c746ddc9a1ad5571a4bdc977f2b0fcd3ad828d30c7fd3fb90"></a>col_dist&#160;</td><td class="fielddoc"><p >Column distributed. </p>
</td></tr>
</table>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="gab037631f28f2fa5d19fd58893c89e986" name="gab037631f28f2fa5d19fd58893c89e986"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gab037631f28f2fa5d19fd58893c89e986">&#9670;&nbsp;</a></span>convert()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class ConversionPolicy , class real_type &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__typedefs.html#ga383790a28074c1998e9abef496429b99">dg::MIHMatrix_t</a>&lt; real_type &gt; dg::convert </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group__typedefs.html#ga405d290c77c92765dcddb324522d8409">dg::IHMatrix_t</a>&lt; real_type &gt; &amp;&#160;</td>
          <td class="paramname"><em>global</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicy &amp;&#160;</td>
          <td class="paramname"><em>policy</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Convert a matrix with local row and global column indices to a row distributed MPI matrix. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">ConversionPolicy</td><td>(can be one of the MPI grids ) has to have the members:<ul>
<li><code>bool<code>global2localIdx(unsigned,unsigned&amp;,unsigned&amp;)</code> <code>const</code>;</code> where the first parameter is the global index and the other two are the output pair (localIdx, rank). return true if successful, false if global index is not part of the grid</li>
<li><code>MPI_Comm</code> <code>communicator</code>() <code>const</code>; returns the communicator to use in the gather/scatter</li>
<li><code>local_size()</code>; return the local vector size </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">global</td><td>the column indices and num_cols need to be global, the row indices and num_rows local </td></tr>
    <tr><td class="paramname">policy</td><td>the conversion object</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a row distributed MPI matrix. If no MPI communication is needed the collective communicator will have zero size. </dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="group__basictopology.html">Topology base classes</a> the MPI grids defined in Level 3 can all be used as a ConversionPolicy </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.9.3-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Fri Jan 14 2022 13:22:07 for Discontinuous Galerkin Library by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.3 </li>
  </ul>
</div>
</body>
</html>
