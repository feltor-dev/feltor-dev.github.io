<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Discontinuous Galerkin Library: MPI backend functionality</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?.../MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
    <!--My own title area-->
  <ul class="tablist"> 
      <!-- <li><a href="../../../index.html">/</a><li>-->
    <li><a href="../../dg/html/modules.html">dG</a></li>
    <li><a href="../../geometries/html/modules.html">geometry</a></li>
    <li><a href="../../file/html/namespacefile.html">file</a></li>
  </ul>
  <!--End My own title area-->
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Discontinuous Galerkin Library
   </div>
   <div id="projectbrief">Discontinuous Galerkin numerical methods and container free numerical algorithms</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('group__mpi__structures.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">MPI backend functionality<div class="ingroups"><a class="el" href="group__backend.html">Level 1: Vectors, Matrices and basic operations</a></div></div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Collaboration diagram for MPI backend functionality:</div>
<div class="dyncontent">
<center><table><tr><td><img src="group__mpi__structures.png" border="0" alt="" usemap="#group____mpi____structures"/>
<map name="group____mpi____structures" id="group____mpi____structures">
<area shape="rect" id="node2" href="group__backend.html" title="Level 1: Vectors, Matrices\l and basic operations" alt="" coords="5,5,180,47"/>
</map>
</td></tr></table></center>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_bijective_comm.html">dg::BijectiveComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Struct that performs bijective collective scatter and gather operations across processes on distributed vectors using mpi.  <a href="structdg_1_1_bijective_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_surjective_comm.html">dg::SurjectiveComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Struct that performs surjective collective scatter and gather operations across processes on distributed vectors using mpi.  <a href="structdg_1_1_surjective_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_general_comm.html">dg::GeneralComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Struct that performs general collective scatter and gather operations across processes on distributed vectors using mpi.  <a href="structdg_1_1_general_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1a_communicator.html">dg::aCommunicator&lt; LocalContainer &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Struct that performs collective scatter and gather operations across processes on distributed vectors using MPI.  <a href="structdg_1_1a_communicator.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_row_col_dist_mat.html">dg::RowColDistMat&lt; LocalMatrixInner, LocalMatrixOuter, Collective &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory matrix class.  <a href="structdg_1_1_row_col_dist_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i_dist_mat.html">dg::MPIDistMat&lt; LocalMatrix, Collective &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distributed memory matrix class.  <a href="structdg_1_1_m_p_i_dist_mat.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_m_p_i___vector.html">dg::MPI_Vector&lt; container &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">mpi Vector class  <a href="structdg_1_1_m_p_i___vector.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structdg_1_1_nearest_neighbor_comm.html">dg::NearestNeighborComm&lt; Index, Vector &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Communicator for nearest neighbor communication.  <a href="structdg_1_1_nearest_neighbor_comm.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">dg::dist_type</a> { <a class="el" href="group__mpi__structures.html#gga521b6d6524a6d391c746ddc9a1ad5571a99ba8c51e9d19b4535a343c4b471a562">dg::row_dist</a> =0, 
<a class="el" href="group__mpi__structures.html#gga521b6d6524a6d391c746ddc9a1ad5571a4bdc977f2b0fcd3ad828d30c7fd3fb90">dg::col_dist</a> =1
 }<tr class="memdesc:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of distribution of MPI distributed matrices.  <a href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:ga521b6d6524a6d391c746ddc9a1ad5571"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga9d33d8a07051c4fe66ceca4a958ebc76"><td class="memTemplParams" colspan="2">template&lt;class ConversionPolicy &gt; </td></tr>
<tr class="memitem:ga9d33d8a07051c4fe66ceca4a958ebc76"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="group__typedefs.html#gabf22476aa30d9a5d7d52a81f97078590">dg::MIHMatrix</a>&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="group__mpi__structures.html#ga9d33d8a07051c4fe66ceca4a958ebc76">dg::convert</a> (const <a class="el" href="group__typedefs.html#ga4780e2f8f426e5e691562dc00b34a9c2">dg::IHMatrix</a> &amp;global, const ConversionPolicy &amp;policy)</td></tr>
<tr class="memdesc:ga9d33d8a07051c4fe66ceca4a958ebc76"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convert a matrix with local row and global column indices to a row distributed MPI matrix.  <a href="group__mpi__structures.html#ga9d33d8a07051c4fe66ceca4a958ebc76">More...</a><br /></td></tr>
<tr class="separator:ga9d33d8a07051c4fe66ceca4a958ebc76"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>In this section the blas functions are implemented for the MPI+X hardware architectures, where X is e.g. CPU, GPU, accelerator cards... The general idea to achieve this is to separate global communication from local computations and thus readily reuse the existing, optimized library for the local part.</p>
<dl class="section note"><dt>Note</dt><dd>The mpi backend is activated by including <code>mpi.h</code> before any other feltor header file </dd></dl>
<h1><a class="anchor" id="mpi_vector"></a>
MPI Vectors and the blas1 functions</h1>
<p>In Feltor each mpi process gets an equally sized chunk of a vector. The corresponding structure in FELTOR is the <code><a class="el" href="structdg_1_1_m_p_i___vector.html" title="mpi Vector class ">dg::MPI_Vector</a></code>, which is nothing but a wrapper around any container type object and a <code>MPI_Comm</code>. With this the <code><a class="el" href="namespacedg_1_1blas1.html" title="BLAS Level 1 routines. ">dg::blas1</a></code> functions can readily implemented by just redirecting to the implementation for the container type. The only functions that need communication are the <code><a class="el" href="group__blas1.html#gab5a71c1ee08c86ec0627a335d28848f4" title="; Euclidean dot product between two containers ">dg::blas1::dot</a></code> functions (<code>MPI_Allreduce</code>).</p>
<h1><a class="anchor" id="mpi_matrix"></a>
Row and column distributed matrices</h1>
<p>Contrary to a vector a matrix can be distributed among processes in two ways: <em>row-distributed</em> and <em>column-distributed</em>. In a row-distributed matrix each process gets the rows of the matrix that correspond to the indices in the vector it holds. In a column-distributed matrix each process gets the columns of the matrix that correspond to the indices in the vector it holds. When we implement a matrix-vector multiplication the order of communication and computation depends on the distribution of the matrix. First, we define the structure <code><a class="el" href="structdg_1_1_m_p_i_dist_mat.html" title="Distributed memory matrix class. ">dg::MPIDistMat</a></code> as a simple a wrapper around a LocalMatrix type object and an instance of a <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>. </p>
<h2><a class="anchor" id="row"></a>
Row distributed</h2>
<p>For the row-distributed matrix each process first has to gather all elements of the input vector it needs to be able to compute the elements of the output. In general this requires MPI communication. (read the documentation of <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code> for more info of how global scatter/gather operations work). Formally, the gather operation can be written as a matrix \(G\) of \(1&#39;\)s and \(0&#39;\)s. After the elements have been gathered into a buffer the local matrix-vector multiplications can be executed. </p><p class="formulaDsp">
\[ M = R\cdot G \]
</p>
<p> where \(R\) is the row-distributed matrix with modified indices and \(G\) is the gather matrix, in which the MPI-communication takes place. The <code><a class="el" href="structdg_1_1_row_col_dist_mat.html" title="Distributed memory matrix class. ">dg::RowColDistMat</a></code> goes one step further and separates the matrix \( R\) into a part that can be computed entirely on the local process and a part that needs communication.</p>
<h2><a class="anchor" id="column"></a>
Column distributed</h2>
<p>In a column distributed matrix the local matrix-vector multiplication can be executed first because each processor already has all vector elements it needs. However the resuling elements have to be communicated back to the process they belong to. Furthermore, a process has to sum all elements it receives from other processes on the same index. This is a scatter and reduce operation and it can be written as a scatter matrix \(S\) (s.a. <code><a class="el" href="structdg_1_1a_communicator.html" title="Struct that performs collective scatter and gather operations across processes on distributed vectors...">dg::aCommunicator</a></code>). The transpose of the scatter matrix is a gather matrix and vice-versa. </p><p class="formulaDsp">
\[ M = S\cdot C \]
</p>
<p> where \(S\) is the scatter matrix and \(C\) is the column distributed matrix with modified indices.</p>
<p>It turns out that a row-distributed matrix can be transposed by transposition of the local matrices and the gather matrix (s.a. <code><a class="el" href="group__lowlevel.html#gaccb5652aaa2d450d97996d4eee6de546" title="Generic matrix transpose method. ">dg::transpose</a></code>). The result is then a column distributed matrix. The transpose of a column distributed matrix is a row-distributed matrix and vice-versa. You can create an MPI row-distributed matrix if you know the global column indices by our <code><a class="el" href="group__mpi__structures.html#ga9d33d8a07051c4fe66ceca4a958ebc76" title="Convert a matrix with local row and global column indices to a row distributed MPI matrix...">dg::convert</a></code> function. </p>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a class="anchor" id="ga521b6d6524a6d391c746ddc9a1ad5571"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="group__mpi__structures.html#ga521b6d6524a6d391c746ddc9a1ad5571">dg::dist_type</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Type of distribution of MPI distributed matrices. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="gga521b6d6524a6d391c746ddc9a1ad5571a99ba8c51e9d19b4535a343c4b471a562"></a>row_dist&#160;</td><td class="fielddoc">
<p>Row distributed. </p>
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="gga521b6d6524a6d391c746ddc9a1ad5571a4bdc977f2b0fcd3ad828d30c7fd3fb90"></a>col_dist&#160;</td><td class="fielddoc">
<p>Column distributed. </p>
</td></tr>
</table>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a class="anchor" id="ga9d33d8a07051c4fe66ceca4a958ebc76"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class ConversionPolicy &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="group__typedefs.html#gabf22476aa30d9a5d7d52a81f97078590">dg::MIHMatrix</a> dg::convert </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="group__typedefs.html#ga4780e2f8f426e5e691562dc00b34a9c2">dg::IHMatrix</a> &amp;&#160;</td>
          <td class="paramname"><em>global</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ConversionPolicy &amp;&#160;</td>
          <td class="paramname"><em>policy</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Convert a matrix with local row and global column indices to a row distributed MPI matrix. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">ConversionPolicy</td><td>has to have the members:<ul>
<li><code>global2localIdx(unsigned,unsigned,unsigned)</code> <code>const</code>; where the first parameter is the global index and the other two are the pair (local idx, rank).</li>
<li><code>MPI_Comm</code> <code>communicator</code>() <code>const</code>; returns the communicator to use in the gather/scatter</li>
<li><code>local_size()</code>; return the local vector size </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">global</td><td>the column indices and num_cols need to be global, the row indices and num_rows local </td></tr>
    <tr><td class="paramname">policy</td><td>the conversion object</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a row distributed MPI matrix. If no MPI communication is needed the collective communicator will have zero size. </dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="group__basictopology.html">Topology base classes</a> the MPI grids defined in Level 3 can all be used as a ConversionPolicy </dd></dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Wed Nov 15 2017 10:37:28 for Discontinuous Galerkin Library by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
